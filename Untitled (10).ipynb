{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856fbbf7-71dc-4a77-bbde-65060debf33b",
   "metadata": {},
   "source": [
    "\n",
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "KNN (K-Nearest Neighbors) is a supervised machine learning algorithm used for both classification and regression tasks. It makes predictions based on the majority class (for classification) or the average of the K-nearest neighbors' values (for regression) in the feature space. The algorithm determines the class or value of a data point by considering the labels or values of its nearest neighbors.\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "The choice of the value for K in KNN is crucial and can impact the model's performance. A smaller K makes the model more sensitive to noise, while a larger K may result in a smoother decision boundary but could overlook local patterns. The optimal K value is often found through experimentation, using techniques such as cross-validation. Common values for K are odd numbers to avoid ties in classification.\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "KNN Classifier: Predicts the class of a data point based on the majority class of its K-nearest neighbors.\n",
    "KNN Regressor: Predicts the continuous value for a data point based on the average value of its K-nearest neighbors.\n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Common performance metrics for KNN include accuracy, precision, recall, F1 score (for classification), and Mean Squared Error (for regression). Cross-validation is often employed to get a more reliable estimate of the model's performance.\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "The curse of dimensionality in KNN refers to the phenomenon where the algorithm's performance degrades as the number of features or dimensions increases. In high-dimensional spaces, the concept of \"closeness\" becomes less meaningful, as distances between points tend to become more uniform, and the risk of overfitting increases.\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "KNN does not handle missing values well, as it relies on distances between data points. Imputation techniques like filling missing values with the mean or median of the feature can be applied before using KNN. Another approach is to use specialized imputation methods or algorithms that can handle missing data.\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "KNN Classifier is suitable for problems where the output is categorical, and the goal is to assign a class label.\n",
    "KNN Regressor is appropriate for problems involving continuous output, where the goal is to predict a numerical value.\n",
    "The choice depends on the nature of the problem and the type of output variable.\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "Strengths: Simple to implement, no training phase, and effective for non-linear relationships.\n",
    "Weaknesses: Sensitive to irrelevant or redundant features, computationally expensive for large datasets, and performance depends on the choice of K.\n",
    "Addressing weaknesses: Feature selection, dimensionality reduction, and optimization of K through cross-validation.\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Euclidean Distance: Represents the straight-line distance between two points in the space.\n",
    "Manhattan Distance: Represents the sum of the absolute differences between the coordinates of two points.\n",
    "The choice depends on the nature of the data and the problem.\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "Feature scaling is essential in KNN because the algorithm is distance-based. Features with larger scales can dominate the distance calculation, leading to biased results. Standardizing or normalizing features ensures that each feature contributes proportionally to the distance metric, improving the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82011b77-758a-42d0-859c-3ba5fa968de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
